# Web Scraping

Web scraping the process of accessing data and informaion from websites. This is simply done by inspecting the HTML behind the webiste and then looking for the tags that hold the information we are looking for. For example, if you are scraping a website that provides daily news, and then you click on one of the news to inspect it, you will notice a structure of HTML tags that contains the data about that news box, then you will know the tags you are looking for to finally extract the information you want from them. In Python this is done using a couple of libraries, some of them are used to connect to the resource like requests and urllib.request, and the others for the actual scrapping like BeautifulSoup. What you do is that you pass the tags that contain the information you need, and you might also need to access the attributes of those tags to get what you need. Then once you get the information you want, you can operate on it the way you want. You should take into consideration the policy of the website when doing scrapping.

When web scraping a page, the page gets downloaded by default by the browser, then we implement a web crawler to extract the information we need. The scrapped content might be parsed, searched, reformatted or even loaded to a database. Among the techniques used in web scraping is human copy-and-paste, where regular people do the job of searching for the required information and then copying and saving it for later use. We also have pattern matching which employs regular expressions matching. HTML parsing relies on the fact that some pages upate dynamically by feeding of data from severs, so it tries to translate them into a relational form. We also have many others like DOM parsing, Vertical aggregation and semantic annotation recognizing and so on.

But when doing web scrapping, we have to consider not to cause harm to the webiste being scrapped because web crawlers can sometimes affect the performance of the websites being scrapped, so many websites have sometimes anti-scraping mechanisms, so we should follow some practices when we consider web scraping. Among these rules is to follow the robot.txt file for websites while scraping which contains specific rules for good behavior. Another technique is to make the crawling slower, by making the crowler sleep for a period of time in order not to affect the performance of the website, this in fact is a try to make the crawler seem a bit more humane. You should also try to change the crawling pattern more often beause some websites can detect repetitive patterns in scraping which might lead to blocking your crowler. It is also recommended to make requests through proxies and to rotate them as needed.
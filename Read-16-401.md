# Read 15 - Class 401

## Data Science Primer

### **1. Bird's Eye View**

machine learning is not only about algorithms, but is also about how and why you are going to use these algorithms, actually, using machine learning is just part of the puzzle, you will usually need to know why you are using a specific approach, and what is your purpose of doing what you are doing. We should also take into consideration that we will usually need to operate on data in order to make it ready before we use it.

Machine learning is about making computers able to learn patterns from data in order to make decisions and predictions, other tasks may not explicitly providing data directly, but computers are expected to help us find patterns group data based on some characterstics they find in the data supplied.

When we deal with machine learning, it is curcial to understand some terminology related to machine learning, for example, when we talk about data, we can think of the features contained in that data, which are just the variables used to train the model, we also have a target variable, which is the actual thing we are trying to predict. Features and the value or the target data that corresponds to that set of features are called an observation or a data point. A model is the set of patterns learned from that data, and the algorithm is a special machine learning process used to train the model. The model gets trained using training data.

We usually have two categories of tasks in machine learning, supervised and unsupervised learning. In supervised machine learning, the algorithm is ususally fed with a labeled data, meaning that we supply the correct answer along with each observation, and this type of macnine learning is usually used for making predictions. When it comes to unsupervised learning, we usually are concerned with automating analysis, meaning that the machine will be fed with unlabaled data, and it should try to find patterns or features in that data.

The main steps to take when embarking on a machine learning process is to explore the data, clean it, then trying to extract new features from already existant ones, then to select the correct algorithm, and lastly to train the model.

### **2. Exploratory Analysis**

Knowing our data beforehand is an important step since it will make us gain hints for the next step which is data cleaning, it will also make it easier to extract new features, as it also will make it easier to communicate results. This includes asking outselves about the number of observations we have, features, data types and if we do or do not have a targe variable.

It is also useful at this step to plot the distribution of our numeric data using histograms in order to find or track outliers, and data that does not make sense, or any potential measurement errors. As it also important to plot categorical values using bar plots, those values come in a limited number of values like for example the gender. Plotting categorical values allows us to track classes that tend to have the least count, since they can cause the model to overfit in some cases, as they can also have a negligible effect.

We can also use segmentations since they allow us to observe the relationship between categorical features and numeric features. They allow us to observe the median, the minimum and the maximum values.

We can also study the correlation between numeric features and other numeric features. For example, of the correlation between one feature and the other is 0.95, it means that these two features are highly correlated and the tend to increase or decrease accordingly. We can make use of heatmaps for such task.

### **3. Data Cleaning**

Having clean data is an important step in building your machine learning process, it makes your workflow smoother, and it also promotes producing better results and less trouble along the way, so it is expected that this process might need more time than the other steps. This process includes removing unwanted observations like duplicate observations and irrelevant ones.

This process also includes fixing structural errors which might be the result of measurement errors, data transfer and so on. Examples on this might include typos or inconsistent capitalization.

We should also take into considertaion the outliers, which are observatoins that tend to be extreme, but this does not mean that they are incorrect, so we should check about them first before removing them.

We also should take into considertion missing data, and to deal with missing data we can either drop the observations that contain missing data, or we can impute the missing values. But both techniques are sub-optimal, because we are actually getting rid of real information that might be useful, also, missing data might sometimes mean different things regarding why it is missing in the first place.

### **4. Feature Engineering**

Feature engineering is all about extracting or creating new freatures from the already provided ones, so in data cleaning we tend to remove or reduce the amount of data, but in feature engineering, we usually tend to increase the amount of data. This process usually requires us to make use of others' expertise about the domain.

Among the heuristics we can use in feature engineering is creating interaction features, for example, they might be the products, sums or even the differences between two features. We simply should look at two or more features, and think if we can combine these two features somehow to produce another feature.

We can also try to combine sparse calsses, those classes that have the few total observations. This might include combining classes that are similar.

When dealing with categorical features, it might be useful sometimes to map the values of those features into numeric or dummy values based on the requirements of the algorithm.

And finally, this process might also include unused features, which are the features that does not make sense to use in our algorithm or any other features you might added during feature engineering.

### **5. Algorithm Selection**

When it comes to linear regression, it might be prone to overfitting. Usually, the model the results from linear regression is simply a linear equation, this equation might be dependent on more than one variable or feature, and if we use all the available data to create our model, each observation will match perfectly. Regularization acts on the coefficients of each variable, so it might damp those large ones, it might remove some of them completely, as it also might tune some others. To do regularization, we can use Lasso regression, Ridge regression, or elastic-net.

We can also use decision trees, and the way they are structured is simply by splitting or distributing data on nodes, and each node represents a new entry point to a whole new path to certain range of values. We can also use tree ensembles which allows us to combine predections from multiple separate models which result in other models like random forests and boosted trees.


### **6. Model Training**

This process includes splitting the data into two parts, a training part and testing part.

We might need to tune our models, and by this we mean to adjust some of the parameters related to our model. But we have two types of parameters, model parameters and hyperparameters. Model parameters are learned attributes that define individual models. Hyperparameters express "higher-level" structural settings for algorithms.

Another technique that can help in tuning our models is called cross-validation, and they way this works is by splitting our training data more than once, and using part of it as a testing set, we keep doing this a number of times, and each time we calculate the performance, then we calculate the average performance across all the folds in our final performance estimate. Then, all we need to do is perform the entire cross-validation loop detailed above on each set of hyperparameter values we'd like to try. Then we finally end up choosing our winning model which performs the best based on specific measures depending on the type of the algorithm and the task, so for example, for regression tasks, it is recommended to use the mean squared error and for classification, it is recommended to use area under ROC curve.




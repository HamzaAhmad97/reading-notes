# Docker

## Docker

We usually face the issue when a team is working on a project, especially when a new member joins the team, it is sometimes hard to reproduce the same setup and configuration. The problem arises when we take into consideration that usually the dependancies that the apps rely on to function might be of vaious versions, the libraries might differ, as well as the operating systems themselves. Part of the team might be using Ubuntu, and the rest might be using Windows and so on. So it becomes harder and harder to match all these requirements to get the app to work.

Docker comes to the rescue. It is a complex tool that allows us to overcomes all of these issues, it allows apps to work or run in kind of a isolated enviremonment so we do not have to worry about the versions of the libraries used or the operating systems themselves. The idea is siimilar to working with virtual environments, but with Docker, the one does not have to run a separate virutal environement every time they want to run the app, and Docker handles all of this for you.

As discussed, the idea comes from virutalization, so for example, when you run a virtual environment on you device, the new virtual environment uses the same hardware, but it is like you made a copy and allocted it to the new virtual environment. Notice that here, an environment refers to some kind of a machine, so it is like having two machines in the same machine, and each one consumes part of the resources available, which might affect the speed and performance of the each other.

We can consider Docker as building that contains a number of apartments, and each apartment has its own utilties, so they do not have to share the same bathroom for example since each one of them has its own. These apratments are the containers that Docker relies on do its work. Apps will run within their own containers so they do not interfere with the others.

With virtual environments, we can only isolate packages, so some apps can for example use a specific version of say Python, and some others might be using a completely different version of Python. But with virtual environments, the apps still are using the underlying system software like databases, which can not be isolated with virtual environments.

To install Docker, just head to their official website, and download the desktop app. You will also need to install another tool called Docker Compose.

An image in Docker is like a blur print or a snapshot of what the project contians, and a container is just a running instance of an image. Within an image, you define how the container should start, and the instructions it needs to follow for the app to function correctly. It is like the image is a set of instructions that tells how the container should organize itself, the thing that makes the app run regardless of the operating system the app is running on, and regarldess of the other libraries or dependancies that other device is using.

To test how Docker works, we can start by creating a new directory, and within this directory, we have to create a special file names **Dockerfile**, this file will act like other more familiar files like _Pipenv_ for example, it just contians the requirements needed to build our image.

The first line in this file must start with `FROM`, which means we want to import a base image to use for our image. These base images can be found on Docker's website and can be used at any time.

```
FROM python:3.7-alpine
```

To build our image, we type the following in the command line interface:

```
docker image build .
```

To check the list of available images you created, type the following in the command line:

```
docker image ls
```

In the Dockerfile, we will usually include more than one line or _step_ that Docker will follow line by line from top to bottom, and these steps are usually referred to as **layers**.

An example on what can go in a Dockerfile can be seen below:

```
# Dockerfile

# Python version
FROM python:3.7-alpine

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

# Set work directory
WORKDIR /code

# Install dependencies
COPY Pipfile Pipfile.lock /code/
RUN pip install pipenv && pipenv install --system

# Copy project
COPY . /code/
```

Then lastely, we head to our Docker compose file, which might include the following:

```
# docker-compose.yml
version: '3.7'

services:
  web:
    build: .
    command: python /code/manage.py runserver 0.0.0.0:8000
    volumes:
      - .:/code
    ports:
      - 8000:8000
```

Then to run the image and run the container, we run:

```
docker-compose up --build
```

# Django for APIs

Note that Django's REST framework works alongside the regular Django web framework, which means that to build an API, Django must be also installed an configured before.

To start, we start a new project and a new app, we do the regular setup like adding the app to our settings, as well as the templates and the urls. We might also need to create a model and a superuser in case we want to control the project from the admin panel. 

Now to start working with Django's REST framework, you will have to install it:

```
poetry add djangorestframework
```
Then we will have to add it to our settings.py as a third party app, for example: `'rest_framework'`. After that, it is preferred if we create a new app named differently like `'api'`, and add it to our apps list in settings.py. This is a good practice so that if we create new apps in the future, each app can contain the models, views, templates and the urls for dedicated webpages. Note that this api app will not have its own database model, so we do not have to migrate. Then we add the app's urls and update the views just like we usually do.

In the views file, we import the class `generics` from `rest_framework`. Also, assuming that for the first app, it was just an app that handles books that have titles, ISPNs and so on. We also import this model from its dedicated models file. Finally, we import `BookSerializer` from `.serializers`. And now our api's views file looks like the following:

```python
from rest_framework import generics

from books.models import Book
from .serializers import BookSerializer


class BookAPIView(generics.ListAPIView):
    queryset = Book.objects.all()
    serializer_class = BookSerializer
```
Note that we use *ListAPIView* since we only need to create a read-only endpoint for the book instances. 

Serializers translate data into formats that are easily to consume over the internet like JSON. Note that we will need to create a file names `serializers.py` within our app's root directory. The file might include something like the following. Note that we specify the fields that we want to expose:

```python
from rest_framework import serializers
from books.models import Book
class BookSerializer(serializers.ModelSerializer):
    class Meta:
        model = Book
        fields = ('title', 'subtitle', 'author', 'isbn')
```
Note that when you open the url in the browser, you will notice that the JSON produced will be formatted and is visualized in a nice way.






